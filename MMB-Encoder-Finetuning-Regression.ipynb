{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "848cf88a-ee06-4c51-8539-c433e831978f",
   "metadata": {},
   "source": [
    "#### https://docs.nvidia.com/bionemo-framework/latest/notebooks/encoder-finetuning-notebook-fw.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f756f2a-e106-446b-933c-aaa81cb0d04a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install openpyxl -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d409b3c9-d3b5-4e72-99c7-479799894f05",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b77c0a53-a227-45ee-9b99-05356e5555b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_excel('TestInputData_June17.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3c80bb1-8eac-4c44-8ca7-69a3bd1371ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Compound as referred to in the document</th>\n",
       "      <th>Canonical Smiles</th>\n",
       "      <th>Size</th>\n",
       "      <th>ExptVariable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Compound 1</td>\n",
       "      <td>CCCCCCCCCCCCCCCCCCN(CCO)CCCCCCCC(=O)OC(CCCCCCC...</td>\n",
       "      <td>72.7</td>\n",
       "      <td>402000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Compound 2</td>\n",
       "      <td>CCCCCCCCCCCCCCN(CCO)CCCCCCCC(=O)OC(CCCCCCCC)CC...</td>\n",
       "      <td>83.9</td>\n",
       "      <td>4870000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Compound 3</td>\n",
       "      <td>CCCCCCCCCN(CCO)CCCCCCCC(=O)OC(CCCCCCCC)CCCCCCCC</td>\n",
       "      <td>97.5</td>\n",
       "      <td>13900000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Compound 4</td>\n",
       "      <td>CCCCCCCCC(CCCCCCCC)OC(=O)CCCCCCCN(CCO)CCCCCCCC</td>\n",
       "      <td>120.5</td>\n",
       "      <td>5260000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Compound 5</td>\n",
       "      <td>CCCCCCCCC(CCCCCCCC)OC(=O)CCCCCCCN(CCO)CCCCCC</td>\n",
       "      <td>196.4</td>\n",
       "      <td>58400000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Compound as referred to in the document  \\\n",
       "0                              Compound 1   \n",
       "1                              Compound 2   \n",
       "2                              Compound 3   \n",
       "3                              Compound 4   \n",
       "4                              Compound 5   \n",
       "\n",
       "                                    Canonical Smiles   Size  ExptVariable  \n",
       "0  CCCCCCCCCCCCCCCCCCN(CCO)CCCCCCCC(=O)OC(CCCCCCC...   72.7     402000000  \n",
       "1  CCCCCCCCCCCCCCN(CCO)CCCCCCCC(=O)OC(CCCCCCCC)CC...   83.9    4870000000  \n",
       "2    CCCCCCCCCN(CCO)CCCCCCCC(=O)OC(CCCCCCCC)CCCCCCCC   97.5   13900000000  \n",
       "3     CCCCCCCCC(CCCCCCCC)OC(=O)CCCCCCCN(CCO)CCCCCCCC  120.5    5260000000  \n",
       "4       CCCCCCCCC(CCCCCCCC)OC(=O)CCCCCCCN(CCO)CCCCCC  196.4      58400000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b179d78-9fb7-41b1-a6da-7d8ed2507a1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = df['Canonical Smiles'].tolist()\n",
    "y = np.log10(df['ExptVariable'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bebb6bae-6162-42aa-ba1c-512539e283f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_prepared = pd.DataFrame({\n",
    "    'SMILES':x,\n",
    "    'y':y,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14d0bd3c-fd1b-4951-bfcf-d4a3e90e9b6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "BIO_HOME = os.environ['BIONEMO_HOME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28261c60-fb91-42cf-96cd-2655ffdf8518",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_path = os.path.join(BIO_HOME,'data/reg_test_input_data','train')\n",
    "test_path = os.path.join(BIO_HOME,'data/reg_test_input_data','test')\n",
    "val_path = os.path.join(BIO_HOME,'data/reg_test_input_data','val')\n",
    "os.makedirs(train_path,exist_ok=True)\n",
    "os.makedirs(test_path,exist_ok=True)\n",
    "os.makedirs(val_path,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0fce253-07d1-45e3-88b1-c0ea48945d22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df,test_df = train_test_split(df_prepared,test_size =0.1,shuffle  = True,random_state= 42)\n",
    "train_df,val_df = train_test_split(df_prepared,test_size =0.1,shuffle  = True,random_state= 42)\n",
    "train_df.to_csv(os.path.join(train_path,'x000.csv'),index = False)\n",
    "test_df.to_csv(os.path.join(test_path,'x000.csv'),index = False)\n",
    "val_df.to_csv(os.path.join(val_path,'x000.csv'),index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2be832d-ba2b-4b28-9a59-d5f5422e69f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99, 12, 12)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df),len(val_df),len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "611f7984-7b7d-4215-bdff-6fae191eadd0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /workspace/bionemo/examples/molecule/megamolbart/conf/finetune_config2.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile /workspace/bionemo/examples/molecule/megamolbart/conf/finetune_config2.yaml\n",
    "name: mmb_physchem\n",
    "defaults: \n",
    "  - pretrain_small_span_aug\n",
    "do_preprocessing: False\n",
    "do_training: True # set to false if data preprocessing steps must be completed\n",
    "do_testing: True # set to true to run evaluation on test data after training, requires test_dataset section\n",
    "restore_from_path: null # path to nemo checkpoint of the fine-tuned model (encoder + task head) to be used for further training, testing or inference\n",
    "target: bionemo.model.molecule.megamolbart.MegaMolBARTModel\n",
    "infer_target: bionemo.model.molecule.megamolbart.infer.MegaMolBARTInference\n",
    "\n",
    "trainer:\n",
    "  devices: 1 # number of GPUs or CPUs\n",
    "  num_nodes: 1\n",
    "  max_epochs: 5 # use max_steps instead with NeMo Megatron models\n",
    "  max_steps: 10 # consumed_samples = global_step * micro_batch_size * data_parallel_size * accumulate_grad_batches\n",
    "  val_check_interval: 1 # set to integer when using steps to determine frequency of validation, use fraction with epochs\n",
    "  limit_val_batches: 1 # number of batches in validation step, use fraction for fraction of data, 0 to disable\n",
    "  limit_test_batches: 1 # number of batches in test step, use fraction for fraction of data, 0 to disable\n",
    "  precision: 16-mixed\n",
    "exp_manager:\n",
    "  wandb_logger_kwargs:\n",
    "    project: ${name}_finetuning\n",
    "    name: ${name}_finetuning_encoder_frozen_${model.encoder_frozen}\n",
    "  checkpoint_callback_params:\n",
    "    monitor: val_loss # use molecular accuracy to select best checkpoints\n",
    "    mode: min # use min or max of monitored metric to select best checkpoints\n",
    "    filename: '${name}-${model.name}--{val_loss:.2f}-{step}-{consumed_samples}'\n",
    "  resume_if_exists: False\n",
    "\n",
    "model:\n",
    "  restore_encoder_path:  '/workspace/bionemo/MegaMolBART_0_2_0.nemo'  #${oc.env:BIONEMO_HOME}/models/molecule/megamolbart/megamolbart.nemo # path to nemo checkpoint of the MegaMolBART model\n",
    "  seq_length: 512 # TODO make a checkpoint with this set to 128. Maximum sequence length allowed. Set to 512 for backwards compatibililty with the checkpoint.\n",
    "  max_position_embeddings: ${.seq_length}\n",
    "  encoder_frozen: True\n",
    "  post_process: False\n",
    "  micro_batch_size: 12 # NOTE: adjust to occupy ~ 90% of GPU memory\n",
    "  global_batch_size: null\n",
    "  tensor_model_parallel_size: 1  # model parallelism\n",
    "  pipeline_model_parallel_size: 1\n",
    "  downstream_task:\n",
    "    n_outputs: 1\n",
    "    hidden_layer_size: 128\n",
    "    loss_func: MSELoss\n",
    "    restore_from_path: '/workspace/bionemo/MegaMolBART_0_2_0.nemo' # path of pretrained model to be used in inference\n",
    "    outputs: [embeddings, hiddens] # Which outputs to extract per sample (a value or list). Possible values: hiddens, embeddings.\n",
    "  data:\n",
    "    # Preprocessing data params\n",
    "    links_file: ${oc.env:BIONEMO_HOME}/examples/molecule/megamolbart/dataset/PhysChem-downloader.txt\n",
    "    preprocessed_data_path: \"/workspace/bionemo/data/\" #sets the location physchem dataset will be downloaded\n",
    "    dataset_path: ${model.data.preprocessed_data_path}/${model.data.task_name} \n",
    "    split_data: False\n",
    "    val_frac: 0.15 # proportion of samples used for validation set\n",
    "    test_frac: 0.15 # proportion of samples used for test set\n",
    "\n",
    "    # Finetuning data params\n",
    "    task_name: reg_test_input_data #specifies which MoleculeNet physchem dataset to use for training, expected values: SAMPL, Lipophilicity, or delaney-processed\n",
    "    task_type: 'regression'\n",
    "    sequence_column: 'SMILES'\n",
    "    target_column: 'y'\n",
    "    emb_batch_size: ${model.micro_batch_size}\n",
    "    dataset:\n",
    "      train: x000\n",
    "      val: x000\n",
    "      test: x000\n",
    "    num_workers: 8\n",
    "    shuffle: False\n",
    "  \n",
    "  finetuning_optim:\n",
    "    name: adam\n",
    "    lr: 0.001\n",
    "    betas:\n",
    "      - 0.9\n",
    "      - 0.999\n",
    "    eps: 1e-8\n",
    "    weight_decay: 0.01\n",
    "    sched:\n",
    "      name: WarmupAnnealing\n",
    "      min_lr: 0.00001\n",
    "      last_epoch: -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e336b1dc-5fa5-4415-91cd-22dc84c93abc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train_script.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train_script.py\n",
    "from nemo.core.config import hydra_runner\n",
    "from nemo.utils import logging\n",
    "from omegaconf.omegaconf import OmegaConf\n",
    "from bionemo.model.utils import (setup_trainer,)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import bionemo.utils\n",
    "from functools import lru_cache\n",
    "from nemo.utils.model_utils import import_class_by_path\n",
    "from bionemo.model.core import MLPModel\n",
    "from bionemo.model.core.encoder_finetuning import EncoderFineTuning\n",
    "\n",
    "#import a BioNeMo data module or your custom data module\n",
    "from bionemo.data.datasets.single_value_dataset import SingleValueDataModule\n",
    "\n",
    "class DownstreamTaskModel(EncoderFineTuning):\n",
    "\n",
    "    def __init__(self, cfg, trainer):\n",
    "        #store config parameters within object so they can be access easily\n",
    "        self.full_cfg = cfg\n",
    "        # we want our downstream model to behave differently based on whether the\n",
    "        # encoder_frozen config parameter is set to True or False so we store it for \n",
    "        # convenient access within the object\n",
    "        self.encoder_frozen = self.full_cfg.model.encoder_frozen\n",
    "        super().__init__(cfg.model, trainer=trainer) \n",
    "        self.batch_target_name = self.cfg.data.target_column\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        super().setup_optimization(optim_config=self.cfg.finetuning_optim)\n",
    "\n",
    "        if self._scheduler is None:\n",
    "            return self._optimizer\n",
    "        else:\n",
    "            return [self._optimizer], [self._scheduler]\n",
    "\n",
    "    # use this function to define what the loss func of the task head should be\n",
    "    def build_loss_fn(self):\n",
    "        return bionemo.utils.lookup_or_use(torch.nn, self.cfg.downstream_task.loss_func)\n",
    "\n",
    "    # define the architecture of our prediction task head for the downstream task\n",
    "    def build_task_head(self):\n",
    "\n",
    "        # we create an instance of MLPModel using parameters defined in the config file\n",
    "        # choose the right task head architecture based on your downstream task (for example,. regression vs classification)\n",
    "        regressor = MLPModel(layer_sizes=[self.encoder_model.cfg.model.hidden_size, self.cfg.downstream_task.hidden_layer_size, self.cfg.downstream_task.n_outputs],\n",
    "            dropout=0.1,\n",
    "        )\n",
    "\n",
    "        # we can use pytorch libraries to further define our architecture and tensor operations\n",
    "        task_head = nn.Sequential(regressor, nn.Flatten(start_dim=0))\n",
    "        return task_head\n",
    "\n",
    "    # returns the model from which we will use the pretrained encoder\n",
    "    def setup_encoder_model(self, cfg, trainer):\n",
    "        infer_class = import_class_by_path(self.full_cfg.infer_target)\n",
    "        pretrained_model = infer_class(\n",
    "            self.full_cfg, \n",
    "            freeze=self.encoder_frozen, #determines whether encoders weights are trainable\n",
    "            restore_path=self.full_cfg.restore_from_path,\n",
    "            training=not self.cfg.encoder_frozen)\n",
    "        return pretrained_model\n",
    "\n",
    "    # use this function to define all your data operations\n",
    "    # in this example, we use the config parameter to determine the value of our model variable\n",
    "    # then we pass it into an instance of SingleValueDataModule()\n",
    "    @lru_cache\n",
    "    def data_setup(self):\n",
    "        if self.encoder_frozen:\n",
    "            model = self.encoder_model\n",
    "        else:\n",
    "            model = None\n",
    "        self.data_module = SingleValueDataModule(\n",
    "            self.cfg, self.trainer, model=model\n",
    "        )\n",
    "\n",
    "    # ensures that we create our necessary datasets \n",
    "    def on_fit_start(self):\n",
    "        self.build_train_valid_test_datasets()\n",
    "        return super().on_fit_start()\n",
    "\n",
    "    # function that simply instatiates our datasets and stores them within our object \n",
    "    def build_train_valid_test_datasets(self):\n",
    "        self._train_ds = self.data_module.get_sampled_train_dataset()\n",
    "        self._validation_ds = self.data_module.get_sampled_val_dataset()\n",
    "        self._test_ds = self.data_module.get_sampled_test_dataset()\n",
    "\n",
    "    # define the behavior for retrieving embeddings from encoder\n",
    "    def encoder_forward(self, bart_model, batch: dict):\n",
    "        if self.encoder_frozen:\n",
    "            enc_output = batch[\"embeddings\"]\n",
    "        else:\n",
    "            enc_output = bart_model.seq_to_embeddings(batch[\"embeddings\"])\n",
    "        return enc_output\n",
    "\n",
    "    # define additional operations on the encoder output\n",
    "    # in this example we simply convert the values of the tensor to float\n",
    "    # see forward() in encoder_finetuning.py for additional information\n",
    "    def extract_for_task_head(self, input_tensor):\n",
    "        return input_tensor.float()\n",
    "    \n",
    "    def get_target_from_batch(self, batch):\n",
    "        ret = batch['target']\n",
    "\n",
    "        return ret.float()\n",
    "\n",
    "@hydra_runner(config_path=\"/workspace/bionemo/examples/molecule/megamolbart/conf/\", config_name=\"finetune_config2\") \n",
    "def main(cfg) -> None:\n",
    "\n",
    "    logging.info(\"\\n\\n************* Finetune config ****************\")\n",
    "    logging.info(f'\\n{OmegaConf.to_yaml(cfg)}')\n",
    "    cfg['exp_manager']['create_wandb_logger'] = False\n",
    "    cfg['model']['downstream_task']['restore_from_path'] = '/workspace/bionemo/MegaMolBART_0_2_0.nemo'\n",
    "    trainer = setup_trainer(\n",
    "        cfg, builder=None,)\n",
    "\n",
    "    # we instantiate our model \n",
    "    model = DownstreamTaskModel(cfg, trainer)\n",
    "\n",
    "    if cfg.do_training:\n",
    "        logging.info(\"************** Starting Training ***********\")\n",
    "        trainer.fit(model) # train our downstream task model using the dataset defined in config\n",
    "        logging.info(\"************** Finished Training ***********\")\n",
    "\n",
    "    if cfg.do_testing:\n",
    "        if \"test\" in cfg.model.data.dataset:\n",
    "            trainer.test(model)\n",
    "        else:\n",
    "            raise UserWarning(\"Skipping testing, test dataset file was not provided. Specify 'test_ds.data_file' in yaml config\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6b8bb5e-887e-4ab8-8b0c-11bffb72b6b1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-06-20 15:48:23 megatron_hiddens:110] Registered hidden transform sampled_var_cond_gaussian at bionemo.model.core.hiddens_support.SampledVarGaussianHiddenTransform\n",
      "[NeMo I 2024-06-20 15:48:23 megatron_hiddens:110] Registered hidden transform interp_var_cond_gaussian at bionemo.model.core.hiddens_support.InterpVarGaussianHiddenTransform\n",
      "[NeMo W 2024-06-20 15:48:24 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'finetune_config2': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information\n",
      "      warnings.warn(msg, UserWarning)\n",
      "    \n",
      "[NeMo W 2024-06-20 15:48:24 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "    See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "      ret = run_job(\n",
      "    \n",
      "[NeMo I 2024-06-20 15:48:24 train_script:110] \n",
      "    \n",
      "    ************* Finetune config ****************\n",
      "[NeMo I 2024-06-20 15:48:24 train_script:111] \n",
      "    name: mmb_physchem\n",
      "    do_training: true\n",
      "    do_testing: true\n",
      "    seed: 42\n",
      "    restore_from_path: null\n",
      "    trainer:\n",
      "      devices: 1\n",
      "      num_nodes: 1\n",
      "      precision: 16-mixed\n",
      "      accelerator: gpu\n",
      "      max_epochs: 100\n",
      "      max_steps: 10000\n",
      "      log_every_n_steps: 100\n",
      "      val_check_interval: 1\n",
      "      num_sanity_val_steps: 2\n",
      "      limit_train_batches: 1.0\n",
      "      limit_val_batches: 1\n",
      "      limit_test_batches: 1\n",
      "      accumulate_grad_batches: 1\n",
      "      gradient_clip_val: 1.0\n",
      "      logger: false\n",
      "      enable_checkpointing: false\n",
      "      use_distributed_sampler: false\n",
      "    model:\n",
      "      name: small_span_aug\n",
      "      micro_batch_size: 12\n",
      "      global_batch_size: null\n",
      "      tensor_model_parallel_size: 1\n",
      "      pipeline_model_parallel_size: 1\n",
      "      resume_from_checkpoint: null\n",
      "      pipeline_model_parallel_split_rank: 0\n",
      "      make_vocab_size_divisible_by: 128\n",
      "      pre_process: true\n",
      "      post_process: false\n",
      "      megatron_amp_O2: false\n",
      "      seq_length: 512\n",
      "      max_position_embeddings: ${.seq_length}\n",
      "      num_layers: 6\n",
      "      hidden_size: 512\n",
      "      ffn_hidden_size: ${multiply:${model.hidden_size}, 4}\n",
      "      num_attention_heads: 8\n",
      "      init_method_std: 0.02\n",
      "      hidden_dropout: 0.1\n",
      "      attention_dropout: 0.1\n",
      "      position_embedding_type: learned_absolute\n",
      "      relative_position_bias_self_attention_only: true\n",
      "      relative_attention_num_buckets: 32\n",
      "      relative_attention_max_distance: 128\n",
      "      kv_channels: null\n",
      "      apply_query_key_layer_scaling: true\n",
      "      layernorm_epsilon: 1.0e-05\n",
      "      persist_layer_norm: true\n",
      "      gradient_as_bucket_view: true\n",
      "      bias_gelu_fusion: true\n",
      "      masked_softmax_fusion: true\n",
      "      bias_dropout_add_fusion: true\n",
      "      bias: true\n",
      "      normalization: layernorm\n",
      "      encoder_arch: transformer\n",
      "      decoder_arch: transformer\n",
      "      activation: gelu\n",
      "      headscale: false\n",
      "      share_token_embeddings: true\n",
      "      share_decoder_tokens_head_embeddings: false\n",
      "      tokenizer:\n",
      "        library: regex\n",
      "        type: null\n",
      "        model: ${oc.env:BIONEMO_HOME}/tokenizers/molecule/megamolbart/vocab/megamolbart.model\n",
      "        vocab_file: ${oc.env:BIONEMO_HOME}/tokenizers/molecule/megamolbart/vocab/megamolbart.vocab\n",
      "        merge_file: null\n",
      "      data:\n",
      "        links_file: ${oc.env:BIONEMO_HOME}/examples/molecule/megamolbart/dataset/PhysChem-downloader.txt\n",
      "        dataset_path: ${model.data.preprocessed_data_path}/${model.data.task_name}\n",
      "        dataset:\n",
      "          train: x000\n",
      "          test: x000\n",
      "          val: x000\n",
      "        canonicalize_target_smile: true\n",
      "        canonicalize_encoder_input: false\n",
      "        canonicalize_decoder_output: false\n",
      "        encoder_augment: true\n",
      "        decoder_independent_augment: false\n",
      "        encoder_mask: true\n",
      "        decoder_mask: false\n",
      "        mask_prob: 0.1\n",
      "        span_lambda: 3.0\n",
      "        micro_batch_size: ${model.micro_batch_size}\n",
      "        num_workers: 8\n",
      "        dataloader_type: single\n",
      "        canonicalize_input: true\n",
      "        max_seq_length: ${model.seq_length}\n",
      "        seed: ${seed}\n",
      "        skip_lines: 0\n",
      "        drop_last: false\n",
      "        pin_memory: false\n",
      "        index_mapping_dir: null\n",
      "        data_impl: csv_mmap\n",
      "        data_impl_kwargs:\n",
      "          csv_mmap:\n",
      "            newline_int: 10\n",
      "            header_lines: 1\n",
      "            workers: ${model.data.num_workers}\n",
      "            sort_dataset_paths: true\n",
      "            data_sep: ','\n",
      "            data_col: 1\n",
      "        use_upsampling: true\n",
      "        preprocessed_data_path: /workspace/bionemo/data/\n",
      "        split_data: false\n",
      "        val_frac: 0.15\n",
      "        test_frac: 0.15\n",
      "        task_name: test_input_data\n",
      "        task_type: regression\n",
      "        sequence_column: SMILES\n",
      "        target_column: 'y'\n",
      "        emb_batch_size: ${model.micro_batch_size}\n",
      "        shuffle: false\n",
      "      optim:\n",
      "        name: fused_adam\n",
      "        lr: 1.0\n",
      "        betas:\n",
      "        - 0.9\n",
      "        - 0.999\n",
      "        eps: 1.0e-08\n",
      "        weight_decay: 0.01\n",
      "        sched:\n",
      "          name: NoamAnnealing\n",
      "          d_model: ${model.hidden_size}\n",
      "          warmup_steps: 8000\n",
      "          warmup_ratio: null\n",
      "          max_steps: 1000000\n",
      "          min_lr: 1.0e-05\n",
      "      dwnstr_task_validation:\n",
      "        enabled: false\n",
      "        dataset:\n",
      "          class: bionemo.model.core.dwnstr_task_callbacks.SingleValuePredictionCallback\n",
      "          task_type: regression\n",
      "          infer_target: bionemo.model.molecule.megamolbart.infer.MegaMolBARTInference\n",
      "          max_seq_length: ${model.seq_length}\n",
      "          emb_batch_size: 128\n",
      "          batch_size: 128\n",
      "          num_epochs: 10\n",
      "          shuffle: true\n",
      "          num_workers: 8\n",
      "          dataset_path: ${oc.env:BIONEMO_HOME}/data/physchem/${model.dwnstr_task_validation.dataset.task_name}\n",
      "          task_name: SAMPL\n",
      "          dataset:\n",
      "            train: x000\n",
      "            test: x000\n",
      "          sequence_column: smiles\n",
      "          target_column: expt\n",
      "          random_seed: 1234\n",
      "          optim:\n",
      "            name: adam\n",
      "            lr: 0.0001\n",
      "            betas:\n",
      "            - 0.9\n",
      "            - 0.999\n",
      "            eps: 1.0e-08\n",
      "            weight_decay: 0.01\n",
      "            sched:\n",
      "              name: WarmupAnnealing\n",
      "              min_lr: 1.0e-05\n",
      "              last_epoch: -1\n",
      "              warmup_ratio: 0.01\n",
      "              max_steps: 1000\n",
      "      restore_encoder_path: /workspace/bionemo/MegaMolBART_0_2_0.nemo\n",
      "      encoder_frozen: true\n",
      "      downstream_task:\n",
      "        n_outputs: 1\n",
      "        hidden_layer_size: 128\n",
      "        loss_func: MSELoss\n",
      "        restore_from_path: /workspace/bionemo/MegaMolBART_0_2_0.nemo\n",
      "        outputs:\n",
      "        - embeddings\n",
      "        - hiddens\n",
      "      finetuning_optim:\n",
      "        name: adam\n",
      "        lr: 0.001\n",
      "        betas:\n",
      "        - 0.9\n",
      "        - 0.999\n",
      "        eps: 1.0e-08\n",
      "        weight_decay: 0.01\n",
      "        sched:\n",
      "          name: WarmupAnnealing\n",
      "          min_lr: 1.0e-05\n",
      "          last_epoch: -1\n",
      "    exp_manager:\n",
      "      name: ${name}\n",
      "      resume_if_exists: true\n",
      "      resume_ignore_no_checkpoint: true\n",
      "      create_checkpoint_callback: true\n",
      "      checkpoint_callback_params:\n",
      "        save_top_k: 3\n",
      "        monitor: val_loss\n",
      "        mode: min\n",
      "        save_last: true\n",
      "        always_save_nemo: true\n",
      "        filename: ${name}-${model.name}--{val_loss:.2f}-{step}-{consumed_samples}\n",
      "        model_parallel_size: ${multiply:${model.tensor_model_parallel_size}, ${model.pipeline_model_parallel_size}}\n",
      "      exp_dir: ${oc.env:BIONEMO_HOME}/results/nemo_experiments/${.name}/${.wandb_logger_kwargs.name}\n",
      "      explicit_log_dir: ${.exp_dir}\n",
      "      create_wandb_logger: true\n",
      "      create_tensorboard_logger: true\n",
      "      wandb_logger_kwargs:\n",
      "        project: ${name}_finetuning\n",
      "        name: ${name}_finetuning_encoder_frozen_${model.encoder_frozen}\n",
      "        group: ${model.name}\n",
      "        job_type: Localhost_nodes_${trainer.num_nodes}_gpus_${trainer.devices}\n",
      "        notes: 'date: ${now:%y%m%d-%H%M%S}'\n",
      "        tags:\n",
      "        - ${name}\n",
      "        - ${model.name}\n",
      "        offline: false\n",
      "    do_preprocessing: false\n",
      "    target: bionemo.model.molecule.megamolbart.MegaMolBARTModel\n",
      "    infer_target: bionemo.model.molecule.megamolbart.infer.MegaMolBARTInference\n",
      "    \n",
      "[NeMo I 2024-06-20 15:48:24 utils:230] Selected Callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]\n",
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
      "`Trainer(limit_val_batches=1)` was configured so 1 batch will be used.\n",
      "`Trainer(limit_test_batches=1)` was configured so 1 batch will be used.\n",
      "`Trainer(val_check_interval=1)` was configured so validation will run after every batch.\n",
      "[NeMo E 2024-06-20 15:48:24 exp_manager:684] exp_manager received explicit_log_dir: /workspace/bionemo/results/nemo_experiments/mmb_physchem/mmb_physchem_finetuning_encoder_frozen_True and at least one of exp_dir: /workspace/bionemo/results/nemo_experiments/mmb_physchem/mmb_physchem_finetuning_encoder_frozen_True, or version: None. Please note that exp_dir, name, and version will be ignored.\n",
      "[NeMo W 2024-06-20 15:48:24 exp_manager:689] Exp_manager is logging to /workspace/bionemo/results/nemo_experiments/mmb_physchem/mmb_physchem_finetuning_encoder_frozen_True, but it already exists.\n",
      "[NeMo I 2024-06-20 15:48:24 exp_manager:639] Resuming training from checkpoint: /workspace/bionemo/results/nemo_experiments/mmb_physchem/mmb_physchem_finetuning_encoder_frozen_True/checkpoints/mmb_physchem-small_span_aug--val_loss=1.03-step=123-consumed_samples=1476.0-last.ckpt\n",
      "[NeMo I 2024-06-20 15:48:24 exp_manager:394] Experiments will be logged at /workspace/bionemo/results/nemo_experiments/mmb_physchem/mmb_physchem_finetuning_encoder_frozen_True\n",
      "[NeMo I 2024-06-20 15:48:24 exp_manager:835] TensorboardLogger has been set up\n",
      "[NeMo W 2024-06-20 15:48:24 exp_manager:931] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to 10000. Please ensure that max_steps will run for at least 1 epochs to ensure that checkpointing will not error out.\n",
      "[NeMo I 2024-06-20 15:48:24 utils:306] \n",
      "    \n",
      "    ************** Trainer configuration ***********\n",
      "[NeMo I 2024-06-20 15:48:24 utils:307] \n",
      "    name: mmb_physchem\n",
      "    do_training: true\n",
      "    do_testing: true\n",
      "    seed: 42\n",
      "    restore_from_path: null\n",
      "    trainer:\n",
      "      devices: 1\n",
      "      num_nodes: 1\n",
      "      precision: 16-mixed\n",
      "      accelerator: gpu\n",
      "      max_epochs: 100\n",
      "      max_steps: 10000\n",
      "      log_every_n_steps: 100\n",
      "      val_check_interval: 1\n",
      "      num_sanity_val_steps: 2\n",
      "      limit_train_batches: 1.0\n",
      "      limit_val_batches: 1\n",
      "      limit_test_batches: 1\n",
      "      accumulate_grad_batches: 1\n",
      "      gradient_clip_val: 1.0\n",
      "      logger: false\n",
      "      enable_checkpointing: false\n",
      "      use_distributed_sampler: false\n",
      "    model:\n",
      "      name: small_span_aug\n",
      "      micro_batch_size: 12\n",
      "      global_batch_size: 12\n",
      "      tensor_model_parallel_size: 1\n",
      "      pipeline_model_parallel_size: 1\n",
      "      resume_from_checkpoint: null\n",
      "      pipeline_model_parallel_split_rank: 0\n",
      "      make_vocab_size_divisible_by: 128\n",
      "      pre_process: true\n",
      "      post_process: false\n",
      "      megatron_amp_O2: false\n",
      "      seq_length: 512\n",
      "      max_position_embeddings: ${.seq_length}\n",
      "      num_layers: 6\n",
      "      hidden_size: 512\n",
      "      ffn_hidden_size: ${multiply:${model.hidden_size}, 4}\n",
      "      num_attention_heads: 8\n",
      "      init_method_std: 0.02\n",
      "      hidden_dropout: 0.1\n",
      "      attention_dropout: 0.1\n",
      "      position_embedding_type: learned_absolute\n",
      "      relative_position_bias_self_attention_only: true\n",
      "      relative_attention_num_buckets: 32\n",
      "      relative_attention_max_distance: 128\n",
      "      kv_channels: null\n",
      "      apply_query_key_layer_scaling: true\n",
      "      layernorm_epsilon: 1.0e-05\n",
      "      persist_layer_norm: true\n",
      "      gradient_as_bucket_view: true\n",
      "      bias_gelu_fusion: true\n",
      "      masked_softmax_fusion: true\n",
      "      bias_dropout_add_fusion: true\n",
      "      bias: true\n",
      "      normalization: layernorm\n",
      "      encoder_arch: transformer\n",
      "      decoder_arch: transformer\n",
      "      activation: gelu\n",
      "      headscale: false\n",
      "      share_token_embeddings: true\n",
      "      share_decoder_tokens_head_embeddings: false\n",
      "      tokenizer:\n",
      "        library: regex\n",
      "        type: null\n",
      "        model: ${oc.env:BIONEMO_HOME}/tokenizers/molecule/megamolbart/vocab/megamolbart.model\n",
      "        vocab_file: ${oc.env:BIONEMO_HOME}/tokenizers/molecule/megamolbart/vocab/megamolbart.vocab\n",
      "        merge_file: null\n",
      "      data:\n",
      "        links_file: ${oc.env:BIONEMO_HOME}/examples/molecule/megamolbart/dataset/PhysChem-downloader.txt\n",
      "        dataset_path: ${model.data.preprocessed_data_path}/${model.data.task_name}\n",
      "        dataset:\n",
      "          train: x000\n",
      "          test: x000\n",
      "          val: x000\n",
      "        canonicalize_target_smile: true\n",
      "        canonicalize_encoder_input: false\n",
      "        canonicalize_decoder_output: false\n",
      "        encoder_augment: true\n",
      "        decoder_independent_augment: false\n",
      "        encoder_mask: true\n",
      "        decoder_mask: false\n",
      "        mask_prob: 0.1\n",
      "        span_lambda: 3.0\n",
      "        micro_batch_size: ${model.micro_batch_size}\n",
      "        num_workers: 8\n",
      "        dataloader_type: single\n",
      "        canonicalize_input: true\n",
      "        max_seq_length: ${model.seq_length}\n",
      "        seed: ${seed}\n",
      "        skip_lines: 0\n",
      "        drop_last: false\n",
      "        pin_memory: false\n",
      "        index_mapping_dir: null\n",
      "        data_impl: csv_mmap\n",
      "        data_impl_kwargs:\n",
      "          csv_mmap:\n",
      "            newline_int: 10\n",
      "            header_lines: 1\n",
      "            workers: ${model.data.num_workers}\n",
      "            sort_dataset_paths: true\n",
      "            data_sep: ','\n",
      "            data_col: 1\n",
      "        use_upsampling: true\n",
      "        preprocessed_data_path: /workspace/bionemo/data/\n",
      "        split_data: false\n",
      "        val_frac: 0.15\n",
      "        test_frac: 0.15\n",
      "        task_name: test_input_data\n",
      "        task_type: regression\n",
      "        sequence_column: SMILES\n",
      "        target_column: 'y'\n",
      "        emb_batch_size: ${model.micro_batch_size}\n",
      "        shuffle: false\n",
      "      optim:\n",
      "        name: fused_adam\n",
      "        lr: 1.0\n",
      "        betas:\n",
      "        - 0.9\n",
      "        - 0.999\n",
      "        eps: 1.0e-08\n",
      "        weight_decay: 0.01\n",
      "        sched:\n",
      "          name: NoamAnnealing\n",
      "          d_model: ${model.hidden_size}\n",
      "          warmup_steps: 8000\n",
      "          warmup_ratio: null\n",
      "          max_steps: 1000000\n",
      "          min_lr: 1.0e-05\n",
      "      dwnstr_task_validation:\n",
      "        enabled: false\n",
      "        dataset:\n",
      "          class: bionemo.model.core.dwnstr_task_callbacks.SingleValuePredictionCallback\n",
      "          task_type: regression\n",
      "          infer_target: bionemo.model.molecule.megamolbart.infer.MegaMolBARTInference\n",
      "          max_seq_length: ${model.seq_length}\n",
      "          emb_batch_size: 128\n",
      "          batch_size: 128\n",
      "          num_epochs: 10\n",
      "          shuffle: true\n",
      "          num_workers: 8\n",
      "          dataset_path: ${oc.env:BIONEMO_HOME}/data/physchem/${model.dwnstr_task_validation.dataset.task_name}\n",
      "          task_name: SAMPL\n",
      "          dataset:\n",
      "            train: x000\n",
      "            test: x000\n",
      "          sequence_column: smiles\n",
      "          target_column: expt\n",
      "          random_seed: 1234\n",
      "          optim:\n",
      "            name: adam\n",
      "            lr: 0.0001\n",
      "            betas:\n",
      "            - 0.9\n",
      "            - 0.999\n",
      "            eps: 1.0e-08\n",
      "            weight_decay: 0.01\n",
      "            sched:\n",
      "              name: WarmupAnnealing\n",
      "              min_lr: 1.0e-05\n",
      "              last_epoch: -1\n",
      "              warmup_ratio: 0.01\n",
      "              max_steps: 1000\n",
      "      restore_encoder_path: /workspace/bionemo/MegaMolBART_0_2_0.nemo\n",
      "      encoder_frozen: true\n",
      "      downstream_task:\n",
      "        n_outputs: 1\n",
      "        hidden_layer_size: 128\n",
      "        loss_func: MSELoss\n",
      "        restore_from_path: /workspace/bionemo/MegaMolBART_0_2_0.nemo\n",
      "        outputs:\n",
      "        - embeddings\n",
      "        - hiddens\n",
      "      finetuning_optim:\n",
      "        name: adam\n",
      "        lr: 0.001\n",
      "        betas:\n",
      "        - 0.9\n",
      "        - 0.999\n",
      "        eps: 1.0e-08\n",
      "        weight_decay: 0.01\n",
      "        sched:\n",
      "          name: WarmupAnnealing\n",
      "          min_lr: 1.0e-05\n",
      "          last_epoch: -1\n",
      "      precision: 16-mixed\n",
      "    exp_manager:\n",
      "      name: ${name}\n",
      "      resume_if_exists: true\n",
      "      resume_ignore_no_checkpoint: true\n",
      "      create_checkpoint_callback: true\n",
      "      checkpoint_callback_params:\n",
      "        save_top_k: 3\n",
      "        monitor: val_loss\n",
      "        mode: min\n",
      "        save_last: true\n",
      "        always_save_nemo: true\n",
      "        filename: ${name}-${model.name}--{val_loss:.2f}-{step}-{consumed_samples}\n",
      "        model_parallel_size: ${multiply:${model.tensor_model_parallel_size}, ${model.pipeline_model_parallel_size}}\n",
      "      exp_dir: ${oc.env:BIONEMO_HOME}/results/nemo_experiments/${.name}/${.wandb_logger_kwargs.name}\n",
      "      explicit_log_dir: ${.exp_dir}\n",
      "      create_wandb_logger: false\n",
      "      create_tensorboard_logger: true\n",
      "      wandb_logger_kwargs:\n",
      "        project: ${name}_finetuning\n",
      "        name: ${name}_finetuning_encoder_frozen_${model.encoder_frozen}\n",
      "        group: ${model.name}\n",
      "        job_type: Localhost_nodes_${trainer.num_nodes}_gpus_${trainer.devices}\n",
      "        notes: 'date: ${now:%y%m%d-%H%M%S}'\n",
      "        tags:\n",
      "        - ${name}\n",
      "        - ${model.name}\n",
      "        offline: false\n",
      "    do_preprocessing: false\n",
      "    target: bionemo.model.molecule.megamolbart.MegaMolBARTModel\n",
      "    infer_target: bionemo.model.molecule.megamolbart.infer.MegaMolBARTInference\n",
      "    \n",
      "[NeMo I 2024-06-20 15:48:24 utils:333] Restoring model from /workspace/bionemo/MegaMolBART_0_2_0.nemo\n",
      "[NeMo I 2024-06-20 15:48:24 utils:337] Loading model class: bionemo.model.molecule.megamolbart.MegaMolBARTModel\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
      "`Trainer(limit_val_batches=1)` was configured so 1 batch will be used.\n",
      "`Trainer(limit_test_batches=1)` was configured so 1 batch will be used.\n",
      "`Trainer(val_check_interval=1)` was configured so validation will run after every batch.\n",
      "[NeMo E 2024-06-20 15:48:24 exp_manager:684] exp_manager received explicit_log_dir: /workspace/bionemo/results/nemo_experiments/mmb_physchem/mmb_physchem_finetuning_encoder_frozen_True and at least one of exp_dir: /workspace/bionemo/results/nemo_experiments/mmb_physchem/mmb_physchem_finetuning_encoder_frozen_True, or version: None. Please note that exp_dir, name, and version will be ignored.\n",
      "[NeMo W 2024-06-20 15:48:24 exp_manager:689] Exp_manager is logging to /workspace/bionemo/results/nemo_experiments/mmb_physchem/mmb_physchem_finetuning_encoder_frozen_True, but it already exists.\n",
      "[NeMo I 2024-06-20 15:48:24 exp_manager:639] Resuming training from checkpoint: /workspace/bionemo/results/nemo_experiments/mmb_physchem/mmb_physchem_finetuning_encoder_frozen_True/checkpoints/mmb_physchem-small_span_aug--val_loss=1.03-step=123-consumed_samples=1476.0-last.ckpt\n",
      "[NeMo I 2024-06-20 15:48:24 exp_manager:394] Experiments will be logged at /workspace/bionemo/results/nemo_experiments/mmb_physchem/mmb_physchem_finetuning_encoder_frozen_True\n",
      "[NeMo I 2024-06-20 15:48:24 exp_manager:835] TensorboardLogger has been set up\n",
      "[NeMo W 2024-06-20 15:48:24 exp_manager:931] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to 10000. Please ensure that max_steps will run for at least 1 epochs to ensure that checkpointing will not error out.\n",
      "[NeMo I 2024-06-20 15:48:24 utils:306] \n",
      "    \n",
      "    ************** Trainer configuration ***********\n",
      "[NeMo I 2024-06-20 15:48:25 utils:307] \n",
      "    name: mmb_physchem\n",
      "    do_training: true\n",
      "    do_testing: true\n",
      "    seed: 42\n",
      "    restore_from_path: null\n",
      "    trainer:\n",
      "      devices: 1\n",
      "      num_nodes: 1\n",
      "      precision: 16-mixed\n",
      "      accelerator: gpu\n",
      "      max_epochs: 100\n",
      "      max_steps: 10000\n",
      "      log_every_n_steps: 100\n",
      "      val_check_interval: 1\n",
      "      num_sanity_val_steps: 2\n",
      "      limit_train_batches: 1.0\n",
      "      limit_val_batches: 1\n",
      "      limit_test_batches: 1\n",
      "      accumulate_grad_batches: 1\n",
      "      gradient_clip_val: 1.0\n",
      "      logger: false\n",
      "      enable_checkpointing: false\n",
      "      use_distributed_sampler: false\n",
      "    model:\n",
      "      name: small_span_aug\n",
      "      global_batch_size: 12\n",
      "      micro_batch_size: 12\n",
      "      tensor_model_parallel_size: 1\n",
      "      pipeline_model_parallel_size: 1\n",
      "      resume_from_checkpoint: null\n",
      "      pipeline_model_parallel_split_rank: 0\n",
      "      make_vocab_size_divisible_by: 128\n",
      "      pre_process: true\n",
      "      post_process: false\n",
      "      megatron_amp_O2: false\n",
      "      seq_length: 512\n",
      "      max_position_embeddings: ${.seq_length}\n",
      "      num_layers: 6\n",
      "      hidden_size: 512\n",
      "      ffn_hidden_size: ${multiply:${model.hidden_size}, 4}\n",
      "      num_attention_heads: 8\n",
      "      init_method_std: 0.02\n",
      "      hidden_dropout: 0.1\n",
      "      attention_dropout: 0.1\n",
      "      kv_channels: null\n",
      "      apply_query_key_layer_scaling: true\n",
      "      layernorm_epsilon: 1.0e-05\n",
      "      persist_layer_norm: true\n",
      "      gradient_as_bucket_view: true\n",
      "      bias_gelu_fusion: true\n",
      "      masked_softmax_fusion: true\n",
      "      bias_dropout_add_fusion: true\n",
      "      bias: true\n",
      "      normalization: layernorm\n",
      "      encoder_arch: transformer\n",
      "      decoder_arch: transformer\n",
      "      activation: gelu\n",
      "      headscale: false\n",
      "      share_word_embeddings: true\n",
      "      share_decoder_tokens_head_embeddings: false\n",
      "      tokenizer:\n",
      "        library: regex\n",
      "        type: null\n",
      "        model: ${oc.env:BIONEMO_HOME}/tokenizers/molecule/megamolbart/vocab/megamolbart.model\n",
      "        vocab_file: ${oc.env:BIONEMO_HOME}/tokenizers/molecule/megamolbart/vocab/megamolbart.vocab\n",
      "        merge_file: null\n",
      "      data:\n",
      "        dataset_path: ${model.data.preprocessed_data_path}/${model.data.task_name}\n",
      "        dataset:\n",
      "          train: x000\n",
      "          test: x000\n",
      "          val: x000\n",
      "        newline_int: 10\n",
      "        header_lines: 1\n",
      "        data_col: 1\n",
      "        data_sep: ','\n",
      "        sort_dataset_paths: true\n",
      "        skip_lines: 0\n",
      "        micro_batch_size: ${model.micro_batch_size}\n",
      "        encoder_augment: true\n",
      "        encoder_mask: true\n",
      "        decoder_augment: true\n",
      "        decoder_mask: false\n",
      "        canonicalize_input: true\n",
      "        dataloader_type: single\n",
      "        drop_last: false\n",
      "        pin_memory: false\n",
      "        num_workers: 8\n",
      "        links_file: ${oc.env:BIONEMO_HOME}/examples/molecule/megamolbart/dataset/PhysChem-downloader.txt\n",
      "        mask_scheme: span\n",
      "        mask_prob: 0.1\n",
      "        span_lambda: 3.0\n",
      "        num_enumerations: 5\n",
      "        dataset_format: csv\n",
      "        canonicalize_target_smile: true\n",
      "        canonicalize_encoder_input: false\n",
      "        canonicalize_decoder_output: false\n",
      "        decoder_independent_augment: false\n",
      "        max_seq_length: ${model.seq_length}\n",
      "        seed: ${seed}\n",
      "        index_mapping_dir: null\n",
      "        data_impl: csv_mmap\n",
      "        data_impl_kwargs:\n",
      "          csv_mmap:\n",
      "            newline_int: 10\n",
      "            header_lines: 1\n",
      "            workers: ${model.data.num_workers}\n",
      "            sort_dataset_paths: true\n",
      "            data_sep: ','\n",
      "            data_col: 1\n",
      "        use_upsampling: true\n",
      "        preprocessed_data_path: /workspace/bionemo/data/\n",
      "        split_data: false\n",
      "        val_frac: 0.15\n",
      "        test_frac: 0.15\n",
      "        task_name: test_input_data\n",
      "        task_type: regression\n",
      "        sequence_column: SMILES\n",
      "        target_column: 'y'\n",
      "        emb_batch_size: ${model.micro_batch_size}\n",
      "        shuffle: false\n",
      "      optim:\n",
      "        name: fused_adam\n",
      "        lr: 1.0\n",
      "        betas:\n",
      "        - 0.9\n",
      "        - 0.999\n",
      "        eps: 1.0e-08\n",
      "        weight_decay: 0.01\n",
      "        sched:\n",
      "          name: NoamAnnealing\n",
      "          d_model: ${model.hidden_size}\n",
      "          warmup_steps: 8000\n",
      "          warmup_ratio: null\n",
      "          max_steps: 1000000\n",
      "          min_lr: 1.0e-05\n",
      "      precision: 16-mixed\n",
      "      target: nemo_chem.models.megamolbart.megamolbart_model.MegaMolBARTModel\n",
      "      nemo_version: 1.11.0rc0\n",
      "      position_embedding_type: learned_absolute\n",
      "      relative_position_bias_self_attention_only: true\n",
      "      relative_attention_num_buckets: 32\n",
      "      relative_attention_max_distance: 128\n",
      "      share_token_embeddings: true\n",
      "      dwnstr_task_validation:\n",
      "        enabled: false\n",
      "        dataset:\n",
      "          class: bionemo.model.core.dwnstr_task_callbacks.SingleValuePredictionCallback\n",
      "          task_type: regression\n",
      "          infer_target: bionemo.model.molecule.megamolbart.infer.MegaMolBARTInference\n",
      "          max_seq_length: ${model.seq_length}\n",
      "          emb_batch_size: 128\n",
      "          batch_size: 128\n",
      "          num_epochs: 10\n",
      "          shuffle: true\n",
      "          num_workers: 8\n",
      "          dataset_path: ${oc.env:BIONEMO_HOME}/data/physchem/${model.dwnstr_task_validation.dataset.task_name}\n",
      "          task_name: SAMPL\n",
      "          dataset:\n",
      "            train: x000\n",
      "            test: x000\n",
      "          sequence_column: smiles\n",
      "          target_column: expt\n",
      "          random_seed: 1234\n",
      "          optim:\n",
      "            name: adam\n",
      "            lr: 0.0001\n",
      "            betas:\n",
      "            - 0.9\n",
      "            - 0.999\n",
      "            eps: 1.0e-08\n",
      "            weight_decay: 0.01\n",
      "            sched:\n",
      "              name: WarmupAnnealing\n",
      "              min_lr: 1.0e-05\n",
      "              last_epoch: -1\n",
      "              warmup_ratio: 0.01\n",
      "              max_steps: 1000\n",
      "      restore_encoder_path: /workspace/bionemo/MegaMolBART_0_2_0.nemo\n",
      "      encoder_frozen: true\n",
      "      downstream_task:\n",
      "        n_outputs: 1\n",
      "        hidden_layer_size: 128\n",
      "        loss_func: MSELoss\n",
      "        restore_from_path: /workspace/bionemo/MegaMolBART_0_2_0.nemo\n",
      "        outputs:\n",
      "        - embeddings\n",
      "        - hiddens\n",
      "      finetuning_optim:\n",
      "        name: adam\n",
      "        lr: 0.001\n",
      "        betas:\n",
      "        - 0.9\n",
      "        - 0.999\n",
      "        eps: 1.0e-08\n",
      "        weight_decay: 0.01\n",
      "        sched:\n",
      "          name: WarmupAnnealing\n",
      "          min_lr: 1.0e-05\n",
      "          last_epoch: -1\n",
      "    exp_manager:\n",
      "      name: ${name}\n",
      "      resume_if_exists: true\n",
      "      resume_ignore_no_checkpoint: true\n",
      "      create_checkpoint_callback: true\n",
      "      checkpoint_callback_params:\n",
      "        save_top_k: 3\n",
      "        monitor: val_loss\n",
      "        mode: min\n",
      "        save_last: true\n",
      "        always_save_nemo: true\n",
      "        filename: ${name}-${model.name}--{val_loss:.2f}-{step}-{consumed_samples}\n",
      "        model_parallel_size: ${multiply:${model.tensor_model_parallel_size}, ${model.pipeline_model_parallel_size}}\n",
      "      exp_dir: ${oc.env:BIONEMO_HOME}/results/nemo_experiments/${.name}/${.wandb_logger_kwargs.name}\n",
      "      explicit_log_dir: ${.exp_dir}\n",
      "      create_wandb_logger: false\n",
      "      create_tensorboard_logger: true\n",
      "      wandb_logger_kwargs:\n",
      "        project: ${name}_finetuning\n",
      "        name: ${name}_finetuning_encoder_frozen_${model.encoder_frozen}\n",
      "        group: ${model.name}\n",
      "        job_type: Localhost_nodes_${trainer.num_nodes}_gpus_${trainer.devices}\n",
      "        notes: 'date: ${now:%y%m%d-%H%M%S}'\n",
      "        tags:\n",
      "        - ${name}\n",
      "        - ${model.name}\n",
      "        offline: false\n",
      "    do_preprocessing: false\n",
      "    target: bionemo.model.molecule.megamolbart.MegaMolBARTModel\n",
      "    infer_target: bionemo.model.molecule.megamolbart.infer.MegaMolBARTInference\n",
      "    \n",
      "[NeMo W 2024-06-20 15:48:25 megatron_base_model:821] The model: MegaMolBARTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-06-20 15:48:25 megatron_base_model:821] The model: MegaMolBARTModel() does not have field.name: virtual_pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-06-20 15:48:25 megatron_base_model:821] The model: MegaMolBARTModel() does not have field.name: sequence_parallel in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-06-20 15:48:25 megatron_base_model:821] The model: MegaMolBARTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-06-20 15:48:25 megatron_base_model:821] The model: MegaMolBARTModel() does not have field.name: use_cpu_initialization in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-06-20 15:48:25 megatron_base_model:821] The model: MegaMolBARTModel() does not have field.name: gradient_accumulation_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-06-20 15:48:25 megatron_base_model:821] The model: MegaMolBARTModel() does not have field.name: tp_comm_overlap in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-06-20 15:48:25 megatron_base_model:821] The model: MegaMolBARTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-06-20 15:48:25 megatron_base_model:821] The model: MegaMolBARTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-06-20 15:48:25 megatron_base_model:821] The model: MegaMolBARTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-06-20 15:48:25 megatron_base_model:821] The model: MegaMolBARTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-06-20 15:48:25 megatron_base_model:821] The model: MegaMolBARTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-06-20 15:48:25 megatron_base_model:821] The model: MegaMolBARTModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-06-20 15:48:25 megatron_base_model:821] The model: MegaMolBARTModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-06-20 15:48:25 megatron_base_model:821] The model: MegaMolBARTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-06-20 15:48:25 megatron_lm_encoder_decoder_model:1547] encoder.hidden_size not found in {'name': 'small_span_aug', 'global_batch_size': 12, 'micro_batch_size': 12, 'tensor_model_parallel_size': 1, 'pipeline_model_parallel_size': 1, 'resume_from_checkpoint': None, 'pipeline_model_parallel_split_rank': 0, 'make_vocab_size_divisible_by': 128, 'pre_process': True, 'post_process': False, 'megatron_amp_O2': False, 'seq_length': 512, 'max_position_embeddings': 512, 'num_layers': 6, 'hidden_size': 512, 'ffn_hidden_size': 2048, 'num_attention_heads': 8, 'init_method_std': 0.02, 'hidden_dropout': 0.1, 'attention_dropout': 0.1, 'kv_channels': None, 'apply_query_key_layer_scaling': True, 'layernorm_epsilon': 1e-05, 'persist_layer_norm': True, 'gradient_as_bucket_view': True, 'bias_gelu_fusion': True, 'masked_softmax_fusion': True, 'bias_dropout_add_fusion': True, 'bias': True, 'normalization': 'layernorm', 'encoder_arch': 'transformer', 'decoder_arch': 'transformer', 'activation': 'gelu', 'headscale': False, 'share_word_embeddings': True, 'share_decoder_tokens_head_embeddings': False, 'tokenizer': {'library': 'regex', 'type': None, 'model': '/workspace/bionemo/tokenizers/molecule/megamolbart/vocab/megamolbart.model', 'vocab_file': '/workspace/bionemo/tokenizers/molecule/megamolbart/vocab/megamolbart.vocab', 'merge_file': None}, 'data': {'dataset_path': '/workspace/bionemo/data//test_input_data', 'dataset': {'train': 'x000', 'test': 'x000', 'val': 'x000'}, 'newline_int': 10, 'header_lines': 1, 'data_col': 1, 'data_sep': ',', 'sort_dataset_paths': True, 'skip_lines': 0, 'micro_batch_size': 12, 'encoder_augment': True, 'encoder_mask': True, 'decoder_augment': True, 'decoder_mask': False, 'canonicalize_input': True, 'dataloader_type': 'single', 'drop_last': False, 'pin_memory': False, 'num_workers': 8, 'links_file': '/workspace/bionemo/examples/molecule/megamolbart/dataset/PhysChem-downloader.txt', 'mask_scheme': 'span', 'mask_prob': 0.1, 'span_lambda': 3.0, 'num_enumerations': 5, 'dataset_format': 'csv', 'canonicalize_target_smile': True, 'canonicalize_encoder_input': False, 'canonicalize_decoder_output': False, 'decoder_independent_augment': False, 'max_seq_length': 512, 'seed': 42, 'index_mapping_dir': None, 'data_impl': 'csv_mmap', 'data_impl_kwargs': {'csv_mmap': {'newline_int': 10, 'header_lines': 1, 'workers': 8, 'sort_dataset_paths': True, 'data_sep': ',', 'data_col': 1}}, 'use_upsampling': True, 'preprocessed_data_path': '/workspace/bionemo/data/', 'split_data': False, 'val_frac': 0.15, 'test_frac': 0.15, 'task_name': 'test_input_data', 'task_type': 'regression', 'sequence_column': 'SMILES', 'target_column': 'y', 'emb_batch_size': 12, 'shuffle': False}, 'optim': {'name': 'fused_adam', 'lr': 1.0, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0.01, 'sched': {'name': 'NoamAnnealing', 'd_model': 512, 'warmup_steps': 8000, 'warmup_ratio': None, 'max_steps': 1000000, 'min_lr': 1e-05}}, 'precision': '16-mixed', 'target': 'nemo_chem.models.megamolbart.megamolbart_model.MegaMolBARTModel', 'nemo_version': '1.11.0rc0', 'position_embedding_type': 'learned_absolute', 'relative_position_bias_self_attention_only': True, 'relative_attention_num_buckets': 32, 'relative_attention_max_distance': 128, 'share_token_embeddings': True, 'dwnstr_task_validation': {'enabled': False, 'dataset': {'class': 'bionemo.model.core.dwnstr_task_callbacks.SingleValuePredictionCallback', 'task_type': 'regression', 'infer_target': 'bionemo.model.molecule.megamolbart.infer.MegaMolBARTInference', 'max_seq_length': 512, 'emb_batch_size': 128, 'batch_size': 128, 'num_epochs': 10, 'shuffle': True, 'num_workers': 8, 'dataset_path': '/workspace/bionemo/data/physchem/SAMPL', 'task_name': 'SAMPL', 'dataset': {'train': 'x000', 'test': 'x000'}, 'sequence_column': 'smiles', 'target_column': 'expt', 'random_seed': 1234, 'optim': {'name': 'adam', 'lr': 0.0001, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0.01, 'sched': {'name': 'WarmupAnnealing', 'min_lr': 1e-05, 'last_epoch': -1, 'warmup_ratio': 0.01, 'max_steps': 1000}}}}, 'restore_encoder_path': '/workspace/bionemo/MegaMolBART_0_2_0.nemo', 'encoder_frozen': True, 'downstream_task': {'n_outputs': 1, 'hidden_layer_size': 128, 'loss_func': 'MSELoss', 'restore_from_path': '/workspace/bionemo/MegaMolBART_0_2_0.nemo', 'outputs': ['embeddings', 'hiddens']}, 'finetuning_optim': {'name': 'adam', 'lr': 0.001, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0.01, 'sched': {'name': 'WarmupAnnealing', 'min_lr': 1e-05, 'last_epoch': -1}}}. Set this in model_parallel_config if using pipeline parallelism.\n",
      "[NeMo I 2024-06-20 15:48:25 megatron_init:234] Rank 0 has data parallel group: [0]\n",
      "[NeMo I 2024-06-20 15:48:25 megatron_init:237] All data parallel group ranks: [[0]]\n",
      "[NeMo I 2024-06-20 15:48:25 megatron_init:238] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2024-06-20 15:48:25 megatron_init:246] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2024-06-20 15:48:25 megatron_init:247] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-06-20 15:48:25 megatron_init:257] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2024-06-20 15:48:25 megatron_init:261] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-06-20 15:48:25 megatron_init:262] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2024-06-20 15:48:25 megatron_init:276] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2024-06-20 15:48:25 megatron_init:288] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2024-06-20 15:48:25 megatron_init:294] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-06-20 15:48:25 megatron_init:295] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2024-06-20 15:48:25 megatron_init:296] All embedding group ranks: [[0]]\n",
      "[NeMo I 2024-06-20 15:48:25 megatron_init:297] Rank 0 has embedding rank: 0\n",
      "24-06-20 15:48:25 - PID:5237 - rank:(0, 0, 0, 0) - microbatches.py:39 - INFO - setting number of micro-batches to constant 1\n",
      "[NeMo W 2024-06-20 15:48:25 megatron_base_model:821] The model: MegaMolBARTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-06-20 15:48:25 megatron_base_model:821] The model: MegaMolBARTModel() does not have field.name: virtual_pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-06-20 15:48:25 megatron_base_model:821] The model: MegaMolBARTModel() does not have field.name: sequence_parallel in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-06-20 15:48:25 megatron_base_model:821] The model: MegaMolBARTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-06-20 15:48:25 megatron_base_model:821] The model: MegaMolBARTModel() does not have field.name: use_cpu_initialization in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-06-20 15:48:25 megatron_base_model:821] The model: MegaMolBARTModel() does not have field.name: gradient_accumulation_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-06-20 15:48:25 megatron_base_model:821] The model: MegaMolBARTModel() does not have field.name: tp_comm_overlap in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-06-20 15:48:25 megatron_base_model:821] The model: MegaMolBARTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-06-20 15:48:25 megatron_base_model:821] The model: MegaMolBARTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-06-20 15:48:25 megatron_base_model:821] The model: MegaMolBARTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-06-20 15:48:25 megatron_base_model:821] The model: MegaMolBARTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-06-20 15:48:25 megatron_base_model:821] The model: MegaMolBARTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-06-20 15:48:25 megatron_base_model:821] The model: MegaMolBARTModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-06-20 15:48:25 megatron_base_model:821] The model: MegaMolBARTModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-06-20 15:48:25 megatron_base_model:821] The model: MegaMolBARTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-06-20 15:48:25 megatron_lm_encoder_decoder_model:1547] encoder.hidden_size not found in {'name': 'small_span_aug', 'global_batch_size': 12, 'micro_batch_size': 12, 'tensor_model_parallel_size': 1, 'pipeline_model_parallel_size': 1, 'resume_from_checkpoint': None, 'pipeline_model_parallel_split_rank': 0, 'make_vocab_size_divisible_by': 128, 'pre_process': True, 'post_process': False, 'megatron_amp_O2': False, 'seq_length': 512, 'max_position_embeddings': 512, 'num_layers': 6, 'hidden_size': 512, 'ffn_hidden_size': 2048, 'num_attention_heads': 8, 'init_method_std': 0.02, 'hidden_dropout': 0.1, 'attention_dropout': 0.1, 'kv_channels': None, 'apply_query_key_layer_scaling': True, 'layernorm_epsilon': 1e-05, 'persist_layer_norm': True, 'gradient_as_bucket_view': True, 'bias_gelu_fusion': True, 'masked_softmax_fusion': True, 'bias_dropout_add_fusion': True, 'bias': True, 'normalization': 'layernorm', 'encoder_arch': 'transformer', 'decoder_arch': 'transformer', 'activation': 'gelu', 'headscale': False, 'share_word_embeddings': True, 'share_decoder_tokens_head_embeddings': False, 'tokenizer': {'library': 'regex', 'type': None, 'model': '/workspace/bionemo/tokenizers/molecule/megamolbart/vocab/megamolbart.model', 'vocab_file': '/workspace/bionemo/tokenizers/molecule/megamolbart/vocab/megamolbart.vocab', 'merge_file': None}, 'data': {'dataset_path': '/workspace/bionemo/data//test_input_data', 'dataset': {'train': 'x000', 'test': 'x000', 'val': 'x000'}, 'newline_int': 10, 'header_lines': 1, 'data_col': 1, 'data_sep': ',', 'sort_dataset_paths': True, 'skip_lines': 0, 'micro_batch_size': 12, 'encoder_augment': True, 'encoder_mask': True, 'decoder_augment': True, 'decoder_mask': False, 'canonicalize_input': True, 'dataloader_type': 'single', 'drop_last': False, 'pin_memory': False, 'num_workers': 8, 'links_file': '/workspace/bionemo/examples/molecule/megamolbart/dataset/PhysChem-downloader.txt', 'mask_scheme': 'span', 'mask_prob': 0.1, 'span_lambda': 3.0, 'num_enumerations': 5, 'dataset_format': 'csv', 'canonicalize_target_smile': True, 'canonicalize_encoder_input': False, 'canonicalize_decoder_output': False, 'decoder_independent_augment': False, 'max_seq_length': 512, 'seed': 42, 'index_mapping_dir': None, 'data_impl': 'csv_mmap', 'data_impl_kwargs': {'csv_mmap': {'newline_int': 10, 'header_lines': 1, 'workers': 8, 'sort_dataset_paths': True, 'data_sep': ',', 'data_col': 1}}, 'use_upsampling': True, 'preprocessed_data_path': '/workspace/bionemo/data/', 'split_data': False, 'val_frac': 0.15, 'test_frac': 0.15, 'task_name': 'test_input_data', 'task_type': 'regression', 'sequence_column': 'SMILES', 'target_column': 'y', 'emb_batch_size': 12, 'shuffle': False}, 'optim': {'name': 'fused_adam', 'lr': 1.0, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0.01, 'sched': {'name': 'NoamAnnealing', 'd_model': 512, 'warmup_steps': 8000, 'warmup_ratio': None, 'max_steps': 1000000, 'min_lr': 1e-05}}, 'precision': '16-mixed', 'target': 'nemo_chem.models.megamolbart.megamolbart_model.MegaMolBARTModel', 'nemo_version': '1.11.0rc0', 'position_embedding_type': 'learned_absolute', 'relative_position_bias_self_attention_only': True, 'relative_attention_num_buckets': 32, 'relative_attention_max_distance': 128, 'share_token_embeddings': True, 'dwnstr_task_validation': {'enabled': False, 'dataset': {'class': 'bionemo.model.core.dwnstr_task_callbacks.SingleValuePredictionCallback', 'task_type': 'regression', 'infer_target': 'bionemo.model.molecule.megamolbart.infer.MegaMolBARTInference', 'max_seq_length': 512, 'emb_batch_size': 128, 'batch_size': 128, 'num_epochs': 10, 'shuffle': True, 'num_workers': 8, 'dataset_path': '/workspace/bionemo/data/physchem/SAMPL', 'task_name': 'SAMPL', 'dataset': {'train': 'x000', 'test': 'x000'}, 'sequence_column': 'smiles', 'target_column': 'expt', 'random_seed': 1234, 'optim': {'name': 'adam', 'lr': 0.0001, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0.01, 'sched': {'name': 'WarmupAnnealing', 'min_lr': 1e-05, 'last_epoch': -1, 'warmup_ratio': 0.01, 'max_steps': 1000}}}}, 'restore_encoder_path': '/workspace/bionemo/MegaMolBART_0_2_0.nemo', 'encoder_frozen': True, 'downstream_task': {'n_outputs': 1, 'hidden_layer_size': 128, 'loss_func': 'MSELoss', 'restore_from_path': '/workspace/bionemo/MegaMolBART_0_2_0.nemo', 'outputs': ['embeddings', 'hiddens']}, 'finetuning_optim': {'name': 'adam', 'lr': 0.001, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0.01, 'sched': {'name': 'WarmupAnnealing', 'min_lr': 1e-05, 'last_epoch': -1}}}. Set this in model_parallel_config if using pipeline parallelism.\n",
      "[NeMo W 2024-06-20 15:48:25 modelPT:251] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact for it has already been registered.\n",
      "[NeMo I 2024-06-20 15:48:25 tokenizer_utils:199] Using regex tokenization\n",
      "[NeMo I 2024-06-20 15:48:25 regex_tokenizer:240] Loading vocabulary from file = /workspace/bionemo/tokenizers/molecule/megamolbart/vocab/megamolbart.vocab\n",
      "[NeMo I 2024-06-20 15:48:25 regex_tokenizer:254] Loading regex from file = /workspace/bionemo/tokenizers/molecule/megamolbart/vocab/megamolbart.model\n",
      "[NeMo I 2024-06-20 15:48:25 megatron_base_model:315] Padded vocab_size: 640, original vocab_size: 523, dummy tokens: 117.\n",
      "[NeMo W 2024-06-20 15:48:25 megatron_lm_encoder_decoder_model:240] Could not find encoder or decoder in config. This is probably because of restoring an old checkpoint. Copying shared model configs to encoder and decoder configs.\n",
      "[NeMo W 2024-06-20 15:48:25 megatron_lm_encoder_decoder_model:206] bias_gelu_fusion is deprecated. Please use bias_activation_fusion instead.\n",
      "[NeMo W 2024-06-20 15:48:25 megatron_lm_encoder_decoder_model:206] bias_gelu_fusion is deprecated. Please use bias_activation_fusion instead.\n",
      "[NeMo I 2024-06-20 15:48:25 nlp_overrides:752] Model MegaMolBARTModel was successfully restored from /workspace/bionemo/MegaMolBART_0_2_0.nemo.\n",
      "[NeMo I 2024-06-20 15:48:25 utils:471] DDP is not initialized. Initializing...\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "[NeMo W 2024-06-20 15:48:25 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/apex/transformer/pipeline_parallel/utils.py:81: UserWarning: This function is only for unittest\n",
      "      warnings.warn(\"This function is only for unittest\")\n",
      "    \n",
      "WARNING: Missing key for extracting the sequence in batches! The forward method call will fail! Provide a valid configuration that has this value under model.data.data_fields_map.sequence.\n",
      "WARNING: Missing key for extracting the sequence IDs in batches! The forward method call will fail! Provide a valid configuration that has this value under model.data.data_fields_map.id.\n",
      "[NeMo W 2024-06-20 15:48:26 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nemo/collections/nlp/modules/common/megatron/fused_bias_dropout_add.py:70: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)\n",
      "      return bias_dropout_add_fused_inference_(*args)\n",
      "    \n",
      "[NeMo I 2024-06-20 15:48:27 megatron_lm_encoder_decoder_model:1195] Decoding using the greedy-search method...\n",
      "[NeMo I 2024-06-20 15:48:44 train_script:121] ************** Starting Training ***********\n",
      "[NeMo W 2024-06-20 15:48:44 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:617: UserWarning: Checkpoint directory /workspace/bionemo/results/nemo_experiments/mmb_physchem/mmb_physchem_finetuning_encoder_frozen_True/checkpoints exists and is not empty.\n",
      "      rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "    \n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[NeMo I 2024-06-20 15:48:44 nlp_overrides:150] Configuring DDP for model parallelism.\n",
      "[NeMo I 2024-06-20 15:48:44 modelPT:728] Optimizer config = Adam (\n",
      "    Parameter Group 0\n",
      "        amsgrad: False\n",
      "        betas: [0.9, 0.999]\n",
      "        capturable: False\n",
      "        differentiable: False\n",
      "        eps: 1e-08\n",
      "        foreach: None\n",
      "        fused: None\n",
      "        lr: 0.001\n",
      "        maximize: False\n",
      "        weight_decay: 0.01\n",
      "    )\n",
      "[NeMo I 2024-06-20 15:48:44 lr_scheduler:943] Scheduler \"<nemo.core.optim.lr_scheduler.WarmupAnnealing object at 0x7971200edfc0>\" \n",
      "    will be used during training (effective maximum steps = 10000) - \n",
      "    Parameters : \n",
      "    (min_lr: 1.0e-05\n",
      "    last_epoch: -1\n",
      "    max_steps: 10000\n",
      "    )\n",
      "\n",
      "   | Name                              | Type                                   | Params\n",
      "----------------------------------------------------------------------------------------------\n",
      "0  | encoder_model                     | MegaMolBARTInference                   | 45.1 M\n",
      "1  | encoder_model.model               | MegaMolBARTModel                       | 45.1 M\n",
      "2  | encoder_model.model.enc_dec_model | MegatronTokenLevelEncoderDecoderModule | 45.1 M\n",
      "3  | loss_fn                           | MSELoss                                | 0     \n",
      "4  | task_head                         | Sequential                             | 66.8 K\n",
      "5  | task_head.0                       | MLPModel                               | 66.8 K\n",
      "6  | task_head.0.linear_layers         | ModuleList                             | 65.8 K\n",
      "7  | task_head.0.layer_norm            | LayerNorm                              | 1.0 K \n",
      "8  | task_head.0.act                   | ReLU                                   | 0     \n",
      "9  | task_head.0.dropout               | Dropout                                | 0     \n",
      "10 | task_head.1                       | Flatten                                | 0     \n",
      "----------------------------------------------------------------------------------------------\n",
      "66.8 K    Trainable params\n",
      "45.1 M    Non-trainable params\n",
      "45.1 M    Total params\n",
      "180.499   Total estimated model params size (MB)\n",
      "[NeMo I 2024-06-20 15:48:44 single_value_dataset:44] Reading file /workspace/bionemo/data//test_input_data/train/x000.csv...\n",
      "  0%|                                                     | 0/9 [00:00<?, ?it/s][NeMo W 2024-06-20 15:48:44 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/apex/transformer/pipeline_parallel/utils.py:81: UserWarning: This function is only for unittest\n",
      "      warnings.warn(\"This function is only for unittest\")\n",
      "    \n",
      "100%|█████████████████████████████████████████████| 9/9 [00:00<00:00, 49.11it/s]\n",
      "[NeMo I 2024-06-20 15:48:45 single_value_dataset:44] Reading file /workspace/bionemo/data//test_input_data/val/x000.csv...\n",
      "  0%|                                                     | 0/1 [00:00<?, ?it/s][NeMo W 2024-06-20 15:48:45 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/apex/transformer/pipeline_parallel/utils.py:81: UserWarning: This function is only for unittest\n",
      "      warnings.warn(\"This function is only for unittest\")\n",
      "    \n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 50.41it/s]\n",
      "[NeMo I 2024-06-20 15:48:45 single_value_dataset:44] Reading file /workspace/bionemo/data//test_input_data/test/x000.csv...\n",
      "  0%|                                                     | 0/1 [00:00<?, ?it/s][NeMo W 2024-06-20 15:48:45 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/apex/transformer/pipeline_parallel/utils.py:81: UserWarning: This function is only for unittest\n",
      "      warnings.warn(\"This function is only for unittest\")\n",
      "    \n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 54.12it/s]\n",
      "[NeMo I 2024-06-20 15:48:45 encoder_finetuning:266] Setting up train dataloader with len(len(self._train_ds)): 99 and consumed samples: 1476\n",
      "[NeMo I 2024-06-20 15:48:45 data_samplers:76] Instantiating MegatronPretrainingSampler with total_samples: 99 and consumed_samples: 1476\n",
      "[NeMo I 2024-06-20 15:48:45 encoder_finetuning:275] Setting up validation dataloader with len(len(self._validation_ds)): 12 and consumed samples: 0\n",
      "[NeMo I 2024-06-20 15:48:45 data_samplers:76] Instantiating MegatronPretrainingSampler with total_samples: 12 and consumed_samples: 0\n",
      "[NeMo I 2024-06-20 15:48:45 encoder_finetuning:284] Setting up test dataloader with len(len(self._test_ds)): 12 and consumed samples: 0\n",
      "[NeMo I 2024-06-20 15:48:45 data_samplers:76] Instantiating MegatronPretrainingSampler with total_samples: 12 and consumed_samples: 0\n",
      "Restoring states from the checkpoint path at /workspace/bionemo/results/nemo_experiments/mmb_physchem/mmb_physchem_finetuning_encoder_frozen_True/checkpoints/mmb_physchem-small_span_aug--val_loss=1.03-step=123-consumed_samples=1476.0-last.ckpt\n",
      "Restored all states from the checkpoint at /workspace/bionemo/results/nemo_experiments/mmb_physchem/mmb_physchem_finetuning_encoder_frozen_True/checkpoints/mmb_physchem-small_span_aug--val_loss=1.03-step=123-consumed_samples=1476.0-last.ckpt\n",
      "[NeMo W 2024-06-20 15:48:45 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (9) is smaller than the logging interval Trainer(log_every_n_steps=100). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "Epoch 15:   0%|                                           | 0/9 [00:00<?, ?it/s][NeMo W 2024-06-20 15:48:46 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/training_epoch_loop.py:151: UserWarning: You're resuming from a checkpoint that ended before the epoch ended. This can cause unreliable results if further training is done. Consider using an end-of-epoch checkpoint\n",
      "      rank_zero_warn(\n",
      "    \n",
      "Epoch 99:   0%|                                   | 0/9 [00:00<?, ?it/s, v_num=]`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "Epoch 99:   0%|                                   | 0/9 [00:00<?, ?it/s, v_num=]\n",
      "[NeMo I 2024-06-20 15:49:13 train_script:123] ************** Finished Training ***********\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Restoring states from the checkpoint path at /workspace/bionemo/results/nemo_experiments/mmb_physchem/mmb_physchem_finetuning_encoder_frozen_True/checkpoints/mmb_physchem-small_span_aug--val_loss=1.03-step=123-consumed_samples=1476.0-last.ckpt\n",
      "Loaded model weights from the checkpoint at /workspace/bionemo/results/nemo_experiments/mmb_physchem/mmb_physchem_finetuning_encoder_frozen_True/checkpoints/mmb_physchem-small_span_aug--val_loss=1.03-step=123-consumed_samples=1476.0-last.ckpt\n",
      "Testing: 0it [00:00, ?it/s][NeMo I 2024-06-20 15:49:13 encoder_finetuning:284] Setting up test dataloader with len(len(self._test_ds)): 12 and consumed samples: 0\n",
      "[NeMo I 2024-06-20 15:49:13 data_samplers:76] Instantiating MegatronPretrainingSampler with total_samples: 12 and consumed_samples: 0\n",
      "Testing DataLoader 0: 100%|███████████████████████| 1/1 [00:00<00:00, 16.60it/s][NeMo W 2024-06-20 15:49:13 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:433: PossibleUserWarning: It is recommended to use `self.log('test_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "      warning_cache.warn(\n",
      "    \n",
      "Testing DataLoader 0: 100%|███████████████████████| 1/1 [00:00<00:00, 14.69it/s]\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
      "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0250357389450073    \u001b[0m\u001b[35m \u001b[0m│\n",
      "└───────────────────────────┴───────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "!python train_script.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088a0e96-200e-43db-8246-2bf50170656d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
